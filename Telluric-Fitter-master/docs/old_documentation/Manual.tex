\documentclass{article}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{url}
\usepackage{listings}
\usepackage{fancyvrb}
\providecommand{\e}[1]{\ensuremath{\times 10^{#1}}}

\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    %breaklines=true,
    %prebreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
    %frame=lines,
    showtabs=false,
    showspaces=false,
    showstringspaces=false,
    keywordstyle=\color{red}\bfseries,
    %stringstyle=\color{green!50!black},
    captionpos=t,
    escapeinside={\%*}{*)}
}


\begin{document}

\title{TelFit Manual}
\author{Kevin Gullikson}
\date{\today}
\maketitle



\section{Installation}
\label{sec:installation}
This section will describe the installation procedure for the TelFit code. It is tailored for linux users, although it should work fine on Mac OSX. This program has not been tested on Windows machines, and likely requires modification to work.
\subsection{Dependencies}
There are several dependencies for TelFit, most of which are python packages that can be installed using the python package manager such as 'pip.' The python packages needed are listed below.
\begin{enumerate}
\item numpy v1.6 or greater
\item cython     
\item matplotlib (used for debugging and in the examples)
\item scipy
\item astropy    (used for unit conversion and some constants)
\item lockfile   (allows several instances of TelFit to be run at once)
\item pysynphot v0.7 or greater  (Used to rebin the telluric model)
\item fortranformat (Used to make the input file for LBLRTM with the correct formatting)
\end{enumerate}
The setup.py script (see Section \ref{sec:setup}) requires both numpy and cython, so they must be installed \emph{before this program.} The setup.py script will attempt to find and install the rest of the dependencies, but I strongly recommend installing at least numpy, scipy, and matplotlib before installing TelFit if you do not have them already. If you are new to Python, the Anaconda distribution (\url{http://continuum.io/downloads}) has all of these packaged nicely.

TelFit also requires the LBLRTM version 12.2 telluric modeling code (in fact, this code is just a wrapper to LBLRTM). It is available from \url{http://rtweb.aer.com/lblrtm.html} The package available contains LBLRTM, which does the telluric modeling, a HITRAN molecular line list, and LNFL, which converts the ascii line list into a binary version suitable for LBLRTM. If you downloaded the TelFit source code from \url{http://www.as.utexas.edu/~kgulliks/projects.html}, then the LBLRTM source code is included. However, if you installed from my git repository (\url{https://github.com/kgullikson88/Telluric-Fitter}), then you will have to put the following tarfiles into the top-level directory of TelFit (alongside setup.py).

\begin{itemize}
\item \verb|aerlbl_v12.2.tar.gz|
\item \verb|aer_v_3.2.tar.gz|
\item \verb|aerlnfl_v2.6.tar.gz|
\end{itemize}

In order to compile LBLRTM, you need a fortran compiler on your system. Make sure you have either the Intel Fortran Compiler, or one of the GNU compilers `gfortran' or `g95'.

The setup.py script will handle the installation of LBLRTM (see Section \ref{sec:setup}). There have been occasional issues using the GNU fortran compiler that are fixed with the Intel compiler. While the intel compiler is not required, I recommend installing it if possible. The compiler is available for free for non-commercial use from \url{http://software.intel.com/en-us/non-commercial-software-development}.



\subsection{Setup}
\label{sec:setup}

The primary setup that needs to be done for this code is making and preparing run directories for LBLRTM. This should all be handled by the setup.py script To make everything with default settings you should run the setup script by typing the command:
\begin{lstlisting}[language=bash]
   python setup.py install [--user]
\end{lstlisting}

By default, setup.py will make four running directories called ``rundir1'', ``rundir2'' ``rundir3'', and ``rundir4.'' The number of running directories can be set by editing the value near the top of the setup.py script. These will be placed in the current working directory.  It also runs LNFL, which generates a binary linelist for use by LBLRTM. By default, the setup script will create a linelist suitable for wavelengths from 300 - 5000 nm. LBLRTM will run faster with a smaller linelist, and so you may wish to adjust these values near the top of the setup script to better suit your needs.

The running directories will contain symbolic links to the lblrtm code and the binary linelist generated by LNFL. It will also contain several input files that configure LBLRTM. The TelFit code will adjust these input files as needed, so you should not need to mess with them.

Finally, the setup.py script will ask to append a line to your ~/.bashrc. If you install TelFit into your home directory (using the --user option of setup.py), you should let the script do the setup for you. However, if you run setup.py as super-user, you may run into permission issues, and so you should reply ``no'' to the prompt. If you say no in the prompt (or anything without a ``y'' in it), you should copy-paste the command it prints to the screen right before the prompt before attempting to use TelFit. 


\section{Examples}
\label{sec:examples}

There are several examples included in the TelFit distribution, to make it easier to learn how to the code. Each example has a separate directory, and each directory has a README file explaining how to use the example file. The examples are enumerated below:

\begin{enumerate}
  \item Library Creation: This example demonstrates how to use the MakeModel class to just generate some telluric models. It does not do any fitting.

  \item Fit Example: This example provides a simple and well-annotated code to perform a telluric fit. It plots the results at the end. It takes about 20 minutes to run on my computer.

  \item Full Echelle Fit Example: This is a much more full-featured example of fitting the telluric absorption in echelle data. It includes reading in a fits file using astropy.io.fits, adjusting the atmosphere profile using a GDAS model (see Appendix), and introduces a new algorithm for fitting. Namely, it fits the water column density with a few echelle orders, fits the $O_2$ column density with different orders, and then applies the best-fit values to every echelle order. This is the algorithm I use and works well for removing telluric absorption in orders that have only very weak telluric lines.
\end{enumerate}


\section{Usage}
\label{sec:usage}
In this section, I describe the various functions available to you. Example scripts are also provided in the examples directory. 

\subsection{MakeModel}
\label{subsec:makemodel}
This is the class used to directly interface with LBLRTM. You should not need to use this directly for fitting, but you may wish to use it if you just want a telluric model for certain parameters. It can be useful, for instance, to easily identify how the transmission spectrum varies as you change the abundances of various molecules. You can change the parameters of the model by editing the bottom of the MakeModel.py file. The available functions are provided below.

\begin{itemize}
\item \begin{Verbatim}[commandchars=\\\{\}]
__init__(debug=False, observatory="McDonald", NumRunDirs=4,
   TelluricModelingDirRoot=os.environ["TELLURICMODELING"], 
   nmolecules=12)
\end{Verbatim}
  \begin{itemize}
  \item debug: False by default. If true, it outputs a bunch of information as it goes
  \item NumRunDirs: This code determines which directory to use so that it doesn't interfere with other active modeling processes. This is an old parameter and is not actually used anymore, but is kept so as not to break things for current users. Now, TelFit automatically finds all of the suitable directories.
  \item TelluricModelingRoot: This is the directory which contains all of the running directories. The environment variable should be set by the configure script (see section \ref{sec:setup}).
  \item nmolecules: The number of molecules to use in the telluric model. Warning: the current version of this program will crash with nmolecules $>$ 12 because the default MIPAS mid-latitude atmosphere does not have the 13th molecule. The molecule numbering is the same as for the LBLRTM code, and is reproduced below:
  \begin{enumerate}
                \item   H$_2$O
                \item   CO$_2$
                \item   O$_3$
                \item   N$_2$O
                \item   CO
                \item   CH$_4$
                \item   O$_2$
                \item   NO
                \item   SO$_2$
                \item   NO$_2$
                \item   NH$_3$
                \item   HNO$_3$
                \item   OH
                \item   HF
                \item   HCl
                \item   HBr
                \item   HI
                \item   ClO
                \item   OCS
                \item   H$_2$CO		 
                \item   HOCl
                \item   N$_2$
                \item   HCN
                \item   CH$_3$Cl
                \item   H$_2$O$_2$
                \item   C$_2$H$_2$
                \item   C$_2$H$_6$
                \item   PH$_3$
                \item   COF$_2$
                \item   SF$_6$
                \item   H$_2$S
                \item   HCOOH
                \item   HO$_2$
                \item   O
                \item   ClONO$_2$
                \item   NO+
                \item   HOBr
                \item   C$_2$H$_4$
                \item   CH$_3$O
  \end{enumerate}
  \end{itemize}
  
  \item \begin{Verbatim}[commandchars=\\\{\}]
EditProfile(self, profilename, profile_height, profile_value)
    \end{Verbatim}
   Use this function to give a new atmospheric profile. This will generally give more accurate results than just using the generic mid-latitude profile. Instructions for getting a profile from the Global Data Assimilation System (GDAS) meteorological archive are shown in Appendix \ref{ap:gdas}.
  
  \begin{itemize}
  \item profilename: The name of the profile you want to edit. Available names are:
  \begin{itemize}
    \item Pressure
    \item Temperature
    \item Molecule (choose one from the list above, without subscripts. Will cause an error if you choose a molecule with a molecule number greater than nmolecules)
  \end{itemize}
  \item profile$\_$height: The height of the profile data points (in km)
  \item profile$\_$value: The value of the profile at each point (units vary)
  \end{itemize}
  
  \item \begin{Verbatim}[commandchars=\\\{\}]
MakeModel(self, pressure=795.0, temperature=283.0, lowfreq=4000,
         highfreq=4600, angle=45.0, humidity=50.0, co2=368.5,
         o3=3.9e-2, n2o=0.32, co=0.14, ch4=1.8, o2=2.1e5, 
         no=1.1e-19, so2=1e-4, no2=1e-4, nh3=1e-4, hno3=5.6e-4,
         lat=30.6, alt=2.1, wavegrid=None, resolution=None, 
         save=False, libfile=None)
    \end{Verbatim}
   This function will make a telluric transmission spectrum model with the given parameters.
   
   \begin{itemize}
     
     \item pressure: the pressure at the observatory altitude, in hPa
     \item temperature: the temperature at the observatory altitude, in K
     \item lowfreq: the lower wavenumber of the model to be created (in cm$^{-1}$
     \item highfreq: the upper wavenumber of the model to be created (in cm$^{-1}$
     \item angle: the zenith angle of the telescope at the time of observation
     \item humidity: The relative humidity at the telescope altitude (in percent)
     \item co2 through hno3: The abundances of the indicated molecules at the observatory altitude (in ppmv)
     \item lat: The latitude of the observatory (in degrees)
     \item alt: The altitude of the observatory (in km)
     \item wavegrid: Interpolate the model onto the wavelength (not \emph{wavenumber}) grid.
     \item resolution: Convolve with a Gaussian profile to reduce the resolution of the model (given as $R\equiv \lambda/(\Delta \lambda)$)
     \item save: if True, it will save the output model as a two-column ascii file. The location of the model is printed to the screen.
     \item libfile: a convenience option useful for creating a library of telluric spectra. Give the name of the file with this keyword variable, and the code will append the location of the model to the libfile. This keyword is ignored if save = False
     
     
   \end{itemize}
   
   
\end{itemize}

\subsection{TelluricFitter}
This is the main code, used to fit a telluric model to some data. Before calling Fit(), you \emph{must} at least tell it which variables to fit. I also recommend using the InputData, AdjustValue, SetObservatory and SetBounds methods before fitting. The following methods are the ones useful for actual fitting; there are other methods in the code, but they are not meant for direct use and so I do not document them.


\begin{itemize}

  \item \begin{Verbatim}[commandchars=\\\{\}]
__init__(debug=False, debug_level=2)
    \end{Verbatim}
    This is just the class initialization function.
  
  \begin{itemize}
    \item debug: If True, it prints out a bunch of information, and may generate some plots using matplotlib
    \item debug$\_$level: Ignored if debug=False. Otherwise, determines how much information is printed to the terminal output. Higher numbers (up to 4) increase the verbosity.
  \end{itemize}
  
  
  \item \begin{Verbatim}[commandchars=\\\{\}]
FitVariable(vardict)
    \end{Verbatim}
    Add one or more variables to the list being fit. The input should be a dictionary where the key is the parameter name and the value is the initial guess value for that parameter.
    
  \item \begin{Verbatim}[commandchars=\\\{\}]
AdjustVariable(vardict)
    \end{Verbatim}
    Adjust the value of a parameter, without telling the code to fit that variable. Useful for things like the telescope zenith angle, which will be known much better than the column density of water, for example. The input should be a dictionary where the key is the parameter name and the value is the  value for that parameter. As of TelFit v0.2, setting a variable with AdjustVariable will \emph{unset} the fitting flag. 
    
  \item \begin{Verbatim}[commandchars=\\\{\}]
SetBounds(bounddict)
    \end{Verbatim}
    Set bounds on a variable that you wish to fit. It will let you set bounds on unfit parameters, but will have no effect. The input should be a dictionary with the name of the parameter as the key, and a list with the upper and lower bounds as the value.
    
  \item \begin{Verbatim}[commandchars=\\\{\}]
SetObservatory(observatory)
    \end{Verbatim}
    Set the observatory your observations are from. This just defines the latitude and altitude, which MakeModel needs (See section \ref{subsec:makemodel})
    The input can either be a dictionary with keys of ``latitude'' and ``altitude'' (and the corresponding values), or one of the following preset strings:
    \begin{itemize}
      \item CTIO
      \item La Silla
      \item Paranal
      \item Mauna Kea
      \item McDonald
    \end{itemize}
    
  \item \begin{Verbatim}[commandchars=\\\{\}]
ImportData(data)
    \end{Verbatim}
    Give the fitter the data to be fit. The input should be a DataStructures.xypoint instance (see section \ref{subsec:datastructures})
  
  \item \begin{Verbatim}[commandchars=\\\{\}]
EditAtmosphereProfile(profilename, profile_height, profile_value)
    \end{Verbatim}
    This is identical to the the version in MakeModel (see Section \ref{subsec:makemodel}).
    
  \item \begin{Verbatim}[commandchars=\\\{\}]
IgnoreRegions(region)
    \end{Verbatim}
    Tells the fitter to ignore certain regions of the spectrum in the chi-squared calculation. Useful for stellar or strong interstellar lines. The input should be one of the following:
    \begin{enumerate}
    \item A list of size two, where the elements are the lower and upper wavelengths of the region to ignore (in nm)
    \item A list of lists, where each sublist is as above. This just allows the user to call the function once instead of several times for each region.
    \end{enumerate}
    
  \item \begin{Verbatim}[commandchars=\\\{\}]
Fit(data=None, resolution_fit_mode="gauss", 
    fit_source=False, return_resolution=False, 
    adjust_wave="model", continuum_fit_order=7, 
    wavelength_fit_order=3)
    \end{Verbatim}
    Here is the all-important fit method. Calling this will begin the fitting process, which can take some time. By default, it returns a DataStructures.xypoint instance (see Section \ref{subsec:datastructures}) for the best fit model. The parameters are enumerated below.
    
    \begin{itemize}

      \item data: The data you want to fit, given as a DataStructures.xypoint object (see Section \ref{subsec:datastructures}). Prior to TelFit v0.2, this had to be given separately in the ImportData method.
    
      \item resolution$\_$fit$\_$mode: The method to use for fitting the detector resolution. Choices are ``SVD'' or ``gauss''. SVD performs a singular value decomposition to estimate the instrumental broadening profile. The gauss mode finds the best-fit gaussian instrumental profile. SVD is liable to fit noise for weak lines and/or low S/N data, and is problematic for extremely strong lines as well. It can however be better than the gauss method for intermediate strength lines, and is a little bit faster as well.
      
      \item fit$\_$source: If True, it will generate a savitzky-golay smoothing spline to the data after dividing by the telluric model in each iteration of the fitting loop. This will only work for very rotationally broadened spectra! Setting this true will cause the Fit() method to return the source as a DataStructures.xypoint in addition to the best-fit model.

      \item return$\_$resolution: controls whether the best-fit resolution is returned to the user. One case I have used this for is to fit echelle data of late-type stars by getting all the best-fit parameters from redder orders with more telluric lines and fewer stellar lines, and then applying those atmospheric parameters to the rest of the orders.
      
      \item adjust$\_$wave: Can be either ``data'' or ``model'. If ``data'', then the wavelength fit will edit the wavelength solution of the data to fit the telluric model. If ``model'', it will adjust the wavelengths in the telluric model. Using the ``data'' option can therefore help to wavelength calibrate the data. Note however, that doing so will usually introduce an unphysical (but constant) velocity shift to the data, because the conversion from vacuum to air wavelengths in the model is done in a very approximate manner. The wavelength shift will be of order $\sim 1$ km s$^{-1}$.
      
      \item continuum$\_$fit$\_$order: The polynomial order to use in the continuum fit. The fit is done in each iteration of the telluric fitting loop, and so gets better as the fit converges.
      
      \item wavelength$\_$fit$\_$order: The polynomial order to use in the wavelength adjustment. Note that this is not fitting pixels $\rightarrow$ wavelength, but rather a wavelength shift function (i.e. $\lambda_i \rightarrow \lambda_i + f(\lambda_i)$). The code will not adjust wavelengths by more than 0.1 nm, so it must be calibrated at least that well to begin with!
    
    \end{itemize}
    
  
  
    
    
    
    
\subsection{DataStructures}
\label{subsec:datastructures}
This just provides the xypoint class, which has the following attributes, all of which are numpy.ndarray objects:
\begin{itemize}
  \item x: The wavelength array
  \item y: The flux
  \item cont: The continuum level of the flux
  \item err: The error in each pixel. If not given, this is just taken as the square root of the flux.
\end{itemize}

The xypoint class also has the following convenience methods:
\begin{itemize}

  \item copy: Returns a deep copy of the xypoint object.
  \item size: Returns the size of the xypoint. This is taken from the ``x'' attribute, so may be wrong if they are not all the same size (but something has gone wrong if they aren't)
  \item output(outfilename): Outputs the xypoint object to the given file as a 4-column ascii with the following columns:
  \begin{enumerate}
    \item x (wavelength) array
    \item y (flux) array
    \item continuum array
    \item error array
  \end{enumerate}
  
  
\end{itemize}
  
The xypoint object supports slicing as well, although the output depends on how it is accessed:
\begin{itemize}

  \item xypoint[index]: Returns a tuple of length 4, containing the x,y,cont,and err values at the given index.
  \item xypoint[index1:index2]: Returns an xypoint object which is a copy of the original between the given indices. 
  \item xypoint[list]: Returns an xypoint object identical to the original, but only at the indices given in the list

\end{itemize}




\end{itemize}



\appendix
\section{Using the Global Data Assimilation System meteorological archive}

\label{ap:gdas}

In most cases, a good atmosphere profile goes a long ways to improving the telluric fit. This section explains the usage of the Global Data Assimilation System meteorological archive, and how to input the results into TelFit. These instructions were made on March 31, 2014. If the archive website changes after publication, you are on your own! The steps for getting an appropriate atmosphere profile are listed below:

\begin{enumerate}
  \item Enter the latitude and longitude of the observatory
  \item In the Sounding row, choose the entry titled ``GDAS (1 deg, 3 hourly, Global).'' For observatories in the United States, the NAM sounding data, labeled ``NAM (12km, 3 hourly, U.S.),'' will be slightly more accurate. Hit the ``Go'' button in sounding row.
  \item Choose the appropriate UT date.
  \item Choose the appropriate UT time in the first row.
  \item Choose ``Text only'' in the ``Output Options'' row.
  \item Choose ``Text listing'' in the ``Graphics'' row.
  \item Complete the captcha and hit the ``Get Sounding'' button
  \item Copy the second block of information into a text file. It will look something like this:
  \begin{verbatim}
 PRESS HGT(MSL) TEMP DEW PT  WND DIR  WND SPD
 HPA       M      C     C       DEG     M/S  
 E = Estimated Surface Height

 1015.     0.   27.1   22.8    92.3     4.2
 1000.   130.   25.8   22.0    93.3     4.5
  975.   354.   23.6   21.6    94.6     4.6
  950.   581.   21.6   20.3    98.5     4.7
  925.   813.   20.3   17.6   108.5     4.2
  900.  1049.   18.9   16.0    96.2     3.7
  875.  1291.   17.6   14.4    95.9     3.3
  850.  1539.   16.2   12.8   102.7     2.8
  825.  1793.   14.8   10.7   110.5     2.4
  800.  2053.   13.6    8.2   114.3     1.9
  775.  2320.   12.8    4.5    98.0     1.3
  750.  2595.   12.2   -0.9    39.3     1.5
  725.  2878.   11.4   -5.1    13.8     2.9
  700.  3170.    9.7   -1.9     5.5     4.4
  650.  3781.    5.8   -4.2   358.6     5.8
  600.  4431.    2.5  -12.6   353.8     5.7
  550.  5130.   -0.6  -23.6   338.2     4.6
  500.  5884.   -5.4  -24.9   322.3     5.0
  450.  6701.  -11.1  -22.5   318.2     8.5
  400.  7594.  -17.5  -33.7   314.5    11.2
  350.  8579.  -24.3  -45.5   312.3    12.5
  300.  9682.  -33.4  -48.3   318.2    12.9
  250. 10934.  -43.8  -55.2   318.3    12.7
  200. 12393.  -55.3  -60.3   303.2    12.7
  150. 14165.  -68.9  -73.2   305.7    10.4
  100. 16532.  -75.7  -83.4   318.7     8.4
   50. 20622.  -65.0  -87.3   144.1     2.9
  \end{verbatim}
\end{enumerate}

The text file you created has the pressure, temperature, and dew point at several atmosphere heights. You should read in the appropriate columns in your Python code (NumPy's loadtxt command works well for this). You must do some quick unit conversion and sorting before inputting the atmosphere profile to TelFit. Some sample code to read in the profile, sort it, convert the dew point to a humidity, convert the height to km, and convert the temperature to Kelvin is shown below. Most of the code is self explanatory, except for the conversion from dew point temperature to mixing ratio in ppmv. The equation and constants were taken from a compilation of formulas for humidity, available at \url{http://www.vaisala.com/Vaisala%20Documents/Application%20notes/Humidity_Conversion_Formulas_B210973EN-F.pdf}





\begin{lstlisting}[language=Python, caption={Code to adjust units from GDAS archive}]


      #Read in GDAS atmosphere profile information
      Pres,height,Temp,dew = numpy.loadtxt(atmosphere_fname,
                                           usecols=(0,1,2,3), 
                                           unpack=True)
      sorter = numpy.argsort(height)
      height = height[sorter]
      Pres = Pres[sorter]
      Temp = Temp[sorter]
      dew = dew[sorter]
      
      #Convert dew point temperature to ppmv
      Pw = 6.116441 * 10**(7.591386*Temp/(Temp + 240.7263))
      h2o = Pw / (Pres-Pw) * 1e6
      
      #Unit conversion
      height /= 1000.0
      Temp += 273.15


\end{lstlisting}

Finally, the atmosphere profile should be input to TelFit

\begin{lstlisting}[language=Python]

      fitter.EditAtmosphereProfile("Temperature", height, Temp)
      fitter.EditAtmosphereProfile("Pressure", height, Pres)
      fitter.EditAtmosphereProfile("H2O", height, h2o)
\end{lstlisting}




%\newpage
%\bibliography{references}
\end{document}

